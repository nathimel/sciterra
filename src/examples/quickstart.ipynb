{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "This demo is a minimal working adaptation of the directions from the README. It involves \n",
    "- creating a loading a one-publication atlas from a bibtex file specifying a single reference,\n",
    "- querying SemanticScholar for this publication, obtaining the document embedding, and \n",
    "- expanding the atlas outwards to 1,000 publications via `iterate_expand`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu.\n"
     ]
    }
   ],
   "source": [
    "# Create a cartographer with a Semantic Scholar librarian and a SciBERT vectorizer\n",
    "from sciterra import Cartographer\n",
    "from sciterra.librarians import SemanticScholarLibrarian # or ADSLibrarian\n",
    "from sciterra.vectorization import SciBERTVectorizer # among others\n",
    "\n",
    "crt = Cartographer(\n",
    "    librarian=SemanticScholarLibrarian(),\n",
    "    vectorizer=SciBERTVectorizer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying Semantic Scholar for 1 total papers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress using call_size=10: 100%|██████████| 1/1 [00:02<00:00,  2.83s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 24244.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# Use the cartographer and a bib file to create an atlas\n",
    "atl = crt.bibtex_to_atlas('./example.bib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "embedding documents: 64it [00:00, 251.67it/s]             \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0]), array([], dtype=int64))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get abstracts\n",
    "docs = [atl[identifier].abstract for identifier in atl.ids]\n",
    "\n",
    "# Embed abstracts\n",
    "result = crt.vectorizer.embed_documents(docs)\n",
    "embeddings = result[\"embeddings\"]\n",
    "\n",
    "# depending on the vectorizer, sometimes not all embeddings can be obtained due to out-of-vocab issues\n",
    "success_indices = result[\"success_indices\"] # shape `(len(embeddings),)`\n",
    "fail_indices = result[\"fail_indices\"] # shape `(len(docs) - len(embeddings))`\n",
    "\n",
    "success_indices, fail_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expansion 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "embedding documents: 64it [00:00, 232.51it/s]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expansion will include 291 new publications.\n",
      "Querying Semantic Scholar for 291 total papers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress using call_size=10:  86%|████████▌ | 250/291 [02:41<00:41,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Had to call <function SemanticScholarLibrarian.get_publications.<locals>.get_papers at 0x284c6c7c0> 2 times to get a response.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress using call_size=10: 100%|██████████| 291/291 [03:04<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 291/291 [00:00<00:00, 2941066.18it/s]\n",
      "Recursively creating atlas data directory at atlas.\n",
      "Writing to atlas/publications.pkl.\n",
      "Writing to atlas/projection.pkl.\n",
      "Writing to atlas/bad_ids.pkl.\n",
      "No history to save, skipping.\n",
      "Writing to atlas/center.pkl.\n",
      "18 publications were filtered due to missing crucial data or incorrect field of study. There are now 18 total ids that will be excluded in the future.\n",
      "Found 273 publications not contained in Atlas projection.\n",
      "Embedding 273 total documents.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlas has 292 publications and 1 embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "embedding documents: 320it [01:27,  3.67it/s]                         \n",
      "Overwriting existing file at atlas/publications.pkl.\n",
      "Overwriting existing file at atlas/projection.pkl.\n",
      "Overwriting existing file at atlas/bad_ids.pkl.\n",
      "Overwriting existing file at atlas/center.pkl.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlas has 274 publications and 274 embeddings.\n",
      "Tracking atlas...\n",
      "Calculating degree of convergence for all publications.\n",
      "computing cosine similarity for 274 embeddings with batch size 274.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 312.70it/s]\n",
      "calculating converged kernel size: 100%|██████████| 274/274 [00:00<00:00, 34918.55it/s]\n",
      "Overwriting existing file at atlas/publications.pkl.\n",
      "Overwriting existing file at atlas/projection.pkl.\n",
      "Overwriting existing file at atlas/bad_ids.pkl.\n",
      "Writing to atlas/history.pkl.\n",
      "Overwriting existing file at atlas/center.pkl.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expansion 2\n",
      "-------------------------------\n",
      "Expansion will include 500 new publications.\n",
      "Querying Semantic Scholar for 500 total papers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress using call_size=10: 100%|██████████| 500/500 [02:50<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 3052623.00it/s]\n",
      "Overwriting existing file at atlas/publications.pkl.\n",
      "Overwriting existing file at atlas/projection.pkl.\n",
      "Overwriting existing file at atlas/bad_ids.pkl.\n",
      "No history to save, skipping.\n",
      "Overwriting existing file at atlas/center.pkl.\n",
      "222 publications were filtered due to missing crucial data or incorrect field of study. There are now 240 total ids that will be excluded in the future.\n",
      "Found 278 publications not contained in Atlas projection.\n",
      "Embedding 278 total documents.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlas has 774 publications and 274 embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "embedding documents: 320it [01:28,  3.60it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlas has 552 publications and 552 embeddings.\n",
      "Tracking atlas...\n",
      "Calculating degree of convergence for all publications.\n",
      "computing cosine similarity for 552 embeddings with batch size 552.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 158.11it/s]\n",
      "calculating converged kernel size: 100%|██████████| 552/552 [00:00<00:00, 19278.38it/s]\n",
      "Overwriting existing file at atlas/publications.pkl.\n",
      "Overwriting existing file at atlas/projection.pkl.\n",
      "Overwriting existing file at atlas/bad_ids.pkl.\n",
      "Overwriting existing file at atlas/history.pkl.\n",
      "Overwriting existing file at atlas/center.pkl.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expansion 3\n",
      "-------------------------------\n",
      "Expansion will include 500 new publications.\n",
      "Querying Semantic Scholar for 500 total papers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress using call_size=10:  88%|████████▊ | 440/500 [04:38<00:53,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Had to call <function SemanticScholarLibrarian.get_publications.<locals>.get_papers at 0x284c6e0c0> 2 times to get a response.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress using call_size=10: 100%|██████████| 500/500 [05:09<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 4032984.62it/s]\n",
      "Overwriting existing file at atlas/publications.pkl.\n",
      "Overwriting existing file at atlas/projection.pkl.\n",
      "Overwriting existing file at atlas/bad_ids.pkl.\n",
      "No history to save, skipping.\n",
      "Overwriting existing file at atlas/center.pkl.\n",
      "139 publications were filtered due to missing crucial data or incorrect field of study. There are now 379 total ids that will be excluded in the future.\n",
      "Found 361 publications not contained in Atlas projection.\n",
      "Embedding 361 total documents.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlas has 1052 publications and 552 embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "embedding documents: 384it [01:51,  3.44it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlas has 913 publications and 913 embeddings.\n",
      "Tracking atlas...\n",
      "Calculating degree of convergence for all publications.\n",
      "computing cosine similarity for 913 embeddings with batch size 913.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 83.34it/s]\n",
      "calculating converged kernel size: 100%|██████████| 913/913 [00:00<00:00, 11717.44it/s]\n",
      "Overwriting existing file at atlas/publications.pkl.\n",
      "Overwriting existing file at atlas/projection.pkl.\n",
      "Overwriting existing file at atlas/bad_ids.pkl.\n",
      "Overwriting existing file at atlas/history.pkl.\n",
      "Overwriting existing file at atlas/center.pkl.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expansion 4\n",
      "-------------------------------\n",
      "Expansion will include 500 new publications.\n",
      "Querying Semantic Scholar for 500 total papers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress using call_size=10:   6%|▌         | 30/500 [00:37<11:23,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Had to call <function SemanticScholarLibrarian.get_publications.<locals>.get_papers at 0x2b6e8b740> 2 times to get a response.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress using call_size=10:  12%|█▏        | 60/500 [01:15<10:38,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Had to call <function SemanticScholarLibrarian.get_publications.<locals>.get_papers at 0x2b6e8a020> 2 times to get a response.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress using call_size=10:  14%|█▍        | 70/500 [01:35<11:50,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Had to call <function SemanticScholarLibrarian.get_publications.<locals>.get_papers at 0x2848f84a0> 2 times to get a response.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress using call_size=10:  18%|█▊        | 90/500 [02:04<11:00,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Had to call <function SemanticScholarLibrarian.get_publications.<locals>.get_papers at 0x284c6c7c0> 2 times to get a response.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress using call_size=10:  40%|████      | 200/500 [03:34<05:25,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Had to call <function SemanticScholarLibrarian.get_publications.<locals>.get_papers at 0x284c6d800> 2 times to get a response.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress using call_size=10:  58%|█████▊    | 290/500 [04:53<04:03,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Had to call <function SemanticScholarLibrarian.get_publications.<locals>.get_papers at 0x28528d440> 2 times to get a response.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress using call_size=10:  82%|████████▏ | 410/500 [06:37<02:12,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Had to call <function SemanticScholarLibrarian.get_publications.<locals>.get_papers at 0x28528e520> 3 times to get a response.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress using call_size=10:  94%|█████████▍| 470/500 [07:32<00:37,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Had to call <function SemanticScholarLibrarian.get_publications.<locals>.get_papers at 0x28528e840> 2 times to get a response.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress using call_size=10:  96%|█████████▌| 480/500 [07:52<00:29,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Had to call <function SemanticScholarLibrarian.get_publications.<locals>.get_papers at 0x28528e980> 2 times to get a response.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress using call_size=10: 100%|██████████| 500/500 [08:15<00:00,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Had to call <function SemanticScholarLibrarian.get_publications.<locals>.get_papers at 0x28528ea20> 2 times to get a response.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 3355443.20it/s]\n",
      "Overwriting existing file at atlas/publications.pkl.\n",
      "Overwriting existing file at atlas/projection.pkl.\n",
      "Overwriting existing file at atlas/bad_ids.pkl.\n",
      "No history to save, skipping.\n",
      "Overwriting existing file at atlas/center.pkl.\n",
      "167 publications were filtered due to missing crucial data or incorrect field of study. There are now 546 total ids that will be excluded in the future.\n",
      "Found 333 publications not contained in Atlas projection.\n",
      "Embedding 333 total documents.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlas has 1413 publications and 913 embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "embedding documents: 384it [01:34,  4.06it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlas has 1246 publications and 1246 embeddings.\n",
      "Tracking atlas...\n",
      "Calculating degree of convergence for all publications.\n",
      "computing cosine similarity for 1246 embeddings with batch size 1000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 55.78it/s]\n",
      "calculating converged kernel size: 100%|██████████| 1246/1246 [00:00<00:00, 8390.90it/s]\n",
      "Overwriting existing file at atlas/publications.pkl.\n",
      "Overwriting existing file at atlas/projection.pkl.\n",
      "Overwriting existing file at atlas/bad_ids.pkl.\n",
      "Overwriting existing file at atlas/history.pkl.\n",
      "Overwriting existing file at atlas/center.pkl.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting loop.\n",
      "Tracking atlas...\n",
      "Calculating degree of convergence for all publications.\n",
      "computing cosine similarity for 1246 embeddings with batch size 1000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 111.06it/s]\n",
      "calculating converged kernel size: 100%|██████████| 1246/1246 [00:00<00:00, 8386.33it/s]\n",
      "Overwriting existing file at atlas/publications.pkl.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expansion loop exited with atlas size 1246 after 4 iterations meeting criteria: {'target_size': True, 'max_failed_expansions': False, 'convergence_func': False}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwriting existing file at atlas/projection.pkl.\n",
      "Overwriting existing file at atlas/bad_ids.pkl.\n",
      "Overwriting existing file at atlas/history.pkl.\n",
      "Overwriting existing file at atlas/center.pkl.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sciterra.mapping.atlas.Atlas at 0x2c7901c50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sciterra.mapping.tracing import iterate_expand\n",
    "\n",
    "# Assuming the initial atlas contains just one publication\n",
    "(atl.center, ) = atl.publications.keys()\n",
    "# build out an atlas to contain 1,000 publications, with increasing dissimilarity to the initial publication, saving progress in binary files to the directory named \"atlas\".\n",
    "atl = iterate_expand(\n",
    "    atl=atl,\n",
    "    crt=crt,\n",
    "    atlas_dir=\"atlas\",\n",
    "    target_size=1000,\n",
    "    center=atl.center,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['04da6471743468b6bb1d26dd9a6eac4c03ca73ee']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
