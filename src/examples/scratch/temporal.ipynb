{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotnine as pn\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sciterra.mapping.atlas import Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bibtex_fp = \"hafenLowredshiftLymanLimit2017.bib\"\n",
    "# atlas_dir = \"outputs/atlas_s2/\"\n",
    "# atlas_dir = \"outputs/atlas_s2-7-19-23\"\n",
    "# atlas_dir = \"outputs/atlas_s2-7-21-23_phil/\"\n",
    "atlas_dir = \"outputs/atlas_s2-7-20-23_uncentered_imeletal/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atl = Atlas.load(atlas_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(atl)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can we get information by year?\n",
    "years = dict()\n",
    "papers_per_year = dict()\n",
    "words = Counter()\n",
    "\n",
    "def get_binary_occurences(abstract: str):\n",
    "    \"\"\"Return a Counter that counts up to 1 for each word.\"\"\"\n",
    "    counter = Counter(abstract.split())\n",
    "    for k in counter:\n",
    "        counter[k] = 1\n",
    "    return counter\n",
    "\n",
    "for id in atl.publications:\n",
    "    pub = atl[id]\n",
    "    # there should be functionality to pass to atlas to 'filter if None' that removes all publications / embeddings that have certain attributes None\n",
    "    if pub.publication_date is not None:\n",
    "        year = pub.publication_date.year\n",
    "        if year not in years:\n",
    "            years[year] = Counter()\n",
    "            papers_per_year[year] = 0\n",
    "        years[year] += get_binary_occurences(pub.abstract)\n",
    "        papers_per_year[year] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_per_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the concept counts by the number of papers per year\n",
    "\n",
    "for key in papers_per_year:\n",
    "    counter = years[key]\n",
    "    for word, freq in counter.items():\n",
    "        counter[word] = freq / papers_per_year[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now just remove years that don't have more than N papers?\n",
    "years = {k:v for k,v in years.items() if papers_per_year[k] > 30}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check not normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function that can visualize the changing distribution over referents\n",
    "\n",
    "dfs = []\n",
    "for year, counter in sorted(years.items()):\n",
    "    dfs.append(\n",
    "        pd.DataFrame(\n",
    "        [(k,v,year) for k,v in counter.items()],\n",
    "        columns=[\"word\", \"fraction of papers mentioning\", \"year\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "data = pd.concat(dfs)\n",
    "\n",
    "# lets just look at a few words for now\n",
    "# words = [\n",
    "#     \"learning\",\n",
    "#     \"hydrodynamic\",\n",
    "#     \"simulations\",\n",
    "#     \"neutrino\",\n",
    "#     \"exoplanet\",\n",
    "#     \"dark\",\n",
    "# ]\n",
    "\n",
    "words = [\n",
    "    \"probability\",\n",
    "    \"language\",\n",
    "    \"communication\",\n",
    "    \"vision\",\n",
    "    # \"deep\",\n",
    "    \"learning\",\n",
    "    \"network\",\n",
    "    # \"evolution\",\n",
    "    \"symbolic\",\n",
    "    # \"efficiency\",\n",
    "    # \"machine\",\n",
    "]\n",
    "\n",
    "# the results are pretty weird / counterintuitive for philosophy. We may need to implement a restriction to specific fields; though this will further limit our sample size.\n",
    "# it feels likely that biomedical + computer science + physics + astronomy are the best represented in semantic scholar.\n",
    "# words = [ \n",
    "#     \"philosophy\",\n",
    "#     \"evidence\",\n",
    "#     \"theory\",\n",
    "#     \"epistemic\",\n",
    "#     \"semantic\",\n",
    "#     # confounds\n",
    "#     \"medical\",\n",
    "#     \"prescription\",\n",
    "# ]\n",
    "\n",
    "data = data[data[\"word\"].isin(words)]\n",
    "data\n",
    "\n",
    "(\n",
    "    pn.ggplot(data, pn.aes(x=\"year\", y=\"fraction of papers mentioning\"))\n",
    "    + pn.geom_point(\n",
    "        mapping=pn.aes(\n",
    "            color=\"word\",\n",
    "        ),\n",
    "        alpha=0.4,\n",
    "    )\n",
    "    + pn.geom_line(\n",
    "        mapping=pn.aes(\n",
    "            color=\"word\",\n",
    "        ),\n",
    "        alpha=0.4,\n",
    "    )\n",
    "    + pn.geom_smooth(\n",
    "        mapping=pn.aes(\n",
    "            color=\"word\",\n",
    "        ),\n",
    "        size=3,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# lol bc idk how to fix faster\n",
    "for key in years:\n",
    "    counter = years[key]\n",
    "    for word, freq in counter.items():\n",
    "        counter[word] = freq * papers_per_year[key]\n",
    "\n",
    "dfs = []\n",
    "for year, counter in sorted(years.items()):\n",
    "    dfs.append(\n",
    "        pd.DataFrame(\n",
    "        [(k,v,year) for k,v in counter.items()],\n",
    "        columns=[\"word\", \"number of papers mentioning\", \"year\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "data = pd.concat(dfs)\n",
    "\n",
    "\n",
    "data = data[data[\"word\"].isin(words)]\n",
    "data\n",
    "\n",
    "(\n",
    "    pn.ggplot(data, pn.aes(x=\"year\", y=\"number of papers mentioning\"))\n",
    "    + pn.geom_point(\n",
    "        mapping=pn.aes(\n",
    "            color=\"word\",\n",
    "        )\n",
    "    )\n",
    "    + pn.geom_line(\n",
    "        mapping=pn.aes(\n",
    "            color=\"word\",\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([papers_per_year[k] for k in papers_per_year if papers_per_year[k] > 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(years[2010].items())[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sciterra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
